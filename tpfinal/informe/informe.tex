%% bare_jrnl_compsoc.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE Computer
%% Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/




% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
% The Computer Society usually requires 12pt for submissions.
%
\documentclass[10pt,journal,compsoc]{IEEEtran}

\usepackage[latin1]{inputenc}
\usepackage{graphicx}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[12pt,journal,compsoc]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  % \usepackage[nocompress]{cite}
\else
  % normal IEEE
  % \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
% IEEEtran.cls V1.7 and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%\usepackage[tight,normalsize,sf,SF]{subfigure}
%\else
%\usepackage[tight,footnotesize]{subfigure}
%\fi
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. Computer Society papers
% use a larger font and \sffamily font for their captions, hence the
% additional options needed under compsoc mode. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.


%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false]{caption}
%  \usepackage[font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false]{caption}
%  \usepackage[font=footnotesize]{subfig}
%\fi
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley and Jeff Goldberg.
% This package may be useful when used in conjunction with IEEEtran.cls'
% captionsoff option. Some IEEE journals/societies require that submissions
% have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.3.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% For subfigure.sty:
% \let\MYorigsubfigure\subfigure
% \renewcommand{\subfigure}[2][\relax]{\MYorigsubfigure[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat/subfig command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/endfloat/
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.


% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
%\title{Aplicación visión estéreo en la navegación de robots móviles}
\title{Algoritmos de navegación para robots móviles usando visión estéreo}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{Agustín Olmedo,\IEEEmembership{Estudiante, UBA}\linebreak 
        Amit Stein,\IEEEmembership{Estudiante, UBA,}\linebreak
        y Fernando Bugni, \IEEEmembership{Estudiante, UBA,}% <-this % stops a space
        }
%\IEEEcompsocitemizethanks{}% <-this % stops a space
%\thanks{}%\IEEEcompsocthanksitem M. Shell is with the Department
%of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
%GA, 30332.\protect\\
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
%E-mail: see http://www.michaelshell.org/contact.html
%\IEEEcompsocthanksitem J. Doe and J. Doe are with Anonymous University.
%Manuscript received April 19, 2005; revised January 11, 2007.
% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~6, No.~1, January~2007}%
%{Shell \MakeLowercase{\textit{et al.}}: Algoritmos de navegaciï¿½n para robots mï¿½viles usando visiï¿½n estï¿½reo}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2007 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2007 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society jorunal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEcompsoctitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
\IEEEcompsoctitleabstractindextext{%
\begin{abstract}
%\boldmath
El presente trabajo desarrolla una implemetación para la navegación de un robot utilizando visión estéreo. Al mísmo utiliza conceptos sobre vision computacional: calibración estéreo, mapa de disparidad, rectificación. La implementación fue desarrollada integramente en C++ utilizando la libreria OpenCV, además de otras librerias para la movilidad del robot.
\end{abstract}
% IEEEtran.cls defaults to using nonbold math in the Abstract.
% This preserves the distinction between vectors and scalars. However,
% if the journal you are submitting to favors bold math in the abstract,
% then you can use LaTeX's standard command \boldmath at the very start
% of the abstract to achieve this. Many IEEE journals frown on math
% in the abstract anyway. In particular, the Computer Society does
% not want either math or citations to appear in the abstract.

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Computer Society, \LaTeX, Visión Computacional, OpenCV, Mapas de Disparidad
\end{IEEEkeywords}}


% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEcompsoctitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynotcompsoctitleabstractindextext when compsoc mode
% is not selected <OR> if conference mode is selected - because compsoc
% conference papers position the abstract like regular (non-compsoc)
% papers do!
\IEEEdisplaynotcompsoctitleabstractindextext
% \IEEEdisplaynotcompsoctitleabstractindextext has no effect when using
% compsoc under a non-conference mode.


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

\section{Introducción}

\IEEEPARstart{L}{a} navegación de robots es el área encargada de desarrollar robots capaces de desplazarse de forma autónoma. Esta tecnología se utiliza en distintos ámbitos como por ejemplo:  misiones espaciales, misiones militares, asistencia para personas con discapacidades e inclusive como guias en una exposición.

En particular en el presente trabajo desarrollaremos un algoritmo de navegación de robots utilizando visión estéreo. En la actualidad, existen diversas técnicas de visión por computadora que nos brindan la información necesaria sobre el entorno. Entre ellas se encuentra la visión estéreo que utiliza dos cámaras para obtener datos de los puntos en tres dimensiones. Algunas aplicaciones de estas técnicas son: construcción de mapas de un entorno desconocido, localización, reconstrucción 3D o también reconocimiento de objetos.

%El presente trabajo describe los fundamentos teóricos y prácticos utilizados para lograr la navegación de un robot, en %particular del exabot (poner link, foto o alguna referencia).

El enfoque de este trabajo consiste, principalmente, en utilizar visión estéreo para calcular el mapa de disparidad asociado a la escena capturada por las dos cámaras. Con la información de disparidad podemos obtener la distancia entre el robot y los objetos que luego será utilizada por el robot para decidir el próximo movimiento a realizar. 

Un paper que ha influido fuertemente el presente trabajo fue "Region of Interest in Disparity Mapping for Navigation of Stereo Vision Autonomous Guided Vehicle". El mismo posee un enfoque similar en cuanto a las técnicas de visión utilizadas. Las diferencias radican en la interpretación de la información que nos brindan los mapas de disparidad.

La organización del paper sigue el orden cronológico en que fuimos aplicando los distintos pasos, empezando con la calibración y calibracón estéreo, luego explicamos los conceptos necesarios para entender de que forma utilizamos la visión estéreo para obtener la información necesaria para realizar el algoritmo de navegación propuesto; continuando con la explicación del algoritmo de navegación. Por último, se incluye una sección de experimentos donde se explica brevemente los experimentos realizado, la arquitectura del robot que se ha usado en los experimentos y las limitaciones del algoritmo.

%\IEEEPARstart{L}{a} navegación de robots es el área encargada de desarrollar robots capaces de desplazarse de forma autónoma. %La utilización de esta tecnología se utiliza en distintos ámbitos como por ejemplo:  misiones espaciales, misiones militares, %asistencia para personas con discapacidades e inclusive como guias en una exposición.

%El objetivo del presente trabajo es desarrollar un algoritmo de navegación de robots utilizando visión estéreo. En la %actualidad, existen diversas técnicas de visión por computadora que nos brindan la información necesaria sobre el entorno en %el que se encuentra el robot para que este pueda tomar la mejor decisión posible. Algunos ejemplos de estas técnicas son: %utilización de cámaras tipo eyebird enfocando todo el terreno, generación de mapas a través de las imágenes tomadas o también %reconocimiento de objetos a través de imágenes. El enfoque de este trabajo consiste, principalmente, en utilizar mapas de %disparidad para obtener información sobre la distancia entre el robot y los objetos que luego será utilizada por el robot para %decidir el próximo movimiento a realizar. Un paper que ha influido fuertemente el presente trabajo fue \"Region of Interest in %Disparity Mapping for Navigation of Stereo Vision Autonomous Guided Vehicle\". Este paper posee un enfoque similar a este, en %cuanto a las técnicas de visión utilizadas. Las diferencias radican en la interpretación de la información que nos brindan los %mapas de disparidad.

%El presente trabajo describe los fundamentos teóricos y prácticos utilizados para lograr la navegación de un robot, en %particular del exabot (poner link, foto o alguna referencia). Cabe destacar que el entorno utilizado juega un papel %fundamental en el buen desempeño del algoritmo, utilizamos un entorno llano iluminado para no perder ningún dato al tomar cada %fotografía. La cámara se encuentra en la parte superior del frente del robot y su movilidad se reduce a avanzar, retroceder y %rotar sobre su propio eje.

%Por último, dejaremos ideas para alguna posible continuación.

%Escribir un parrafo de guia de todos los pasos realizados (a grandes rasgos)

%palabras clave: explicacion del problema, estado del arte (antes hablar del arte),
%entorno de prueba, approach del paper, arquitectura de hardware

\section{Calibración de la cámara estéreo}
Cuando capturamos una escena del mundo (tres dimensiones) con una cámara, ésta se proyecta sobre una imagen en dos dimensiones.\\
Para conocer la posición de cada punto es necesario definir un sistema de referencias, es decir, los tres ejes principales con su origen, sus orientaciones y sus unidades.

Existen tres sistemas de referencias posibles:
\begin{itemize}
	\item El sistema del mundo 
	%independiente de la cámara. Unidades físicas, por ejemplo, metros.
	\item El sistema de la cámara
	%origen en el centro óptico. Unidades físicas.
	\item El sistema de la imagen
	%origen en la esquina de la imagen. Unidades en pixel
\end{itemize}

La unidad del sistema de referencia de la imagen es el pixel, mientras que en los demás es el metro. El sistema de la cámara tiene el origen en el centro óptico y el de la imagen en la esquina de la misma.

Los parámetros intrínsecos son los que relacionan el sistema de referencia de la cámara con el de la imagen y los extrínsecos son los que relacionan el sistema de referencia del mundo con el de la cámara.

\begin{figure}[h]
	\centering
		\includegraphics[width=0.50\textwidth]{./figuras/fig10.png}
	\caption{Relación entre sistemas de referencias}
	\label{fig:fig10}
\end{figure}


%\begin{center}
	%Mundo $\stackrel{parametros\\extrinsecos}{\leftrightarrow}$ Cámara $\stackrel{parametros intrinsecos}{\leftrightarrow}$ Imagen
%\end{center}

%El sistema de referencia de la cámara y el sistema de referencia del mundo tienen las mismas unidades. Lo que cambia es el %origen y la orientación de los ejes. Esto significa que la conversión de un sistema al otro  implica una traslación (cambio de %origen) y una rotación (cambio de orientación).Ver figura 1.

La conversión del sistema de referencia del mundo al de la cámara implica una traslación y una rotación.

%\begin{figure}[h]
%	\centering
%		\includegraphics[width=0.50\textwidth]{./figuras/fig9.png}
%		\caption{Parámetros extrínsecos}
%\end{figure}
%el sistema de referencia de la cámara y el sistema de referencia de la imagen no tienen las mismas unidades.
Un punto en el mundo, visto desde la cámara, se proyecta en el plano de la imagen. Geometricamente podemos calcular la relación entre el punto en el mundo y el punto proyectado en el plano de la imagen(Ver figura 2).
Además se traslada el origen del plano de la imagen llevandolo al centro, y se realiza un cambio de unidad dividiendo por el número de pixels por unidad de distancia.

\begin{figure}[h]
	\centering
		\includegraphics[width=0.50\textwidth]{./figuras/fig8.png}
	\caption{\footnotesize\textit{Parámetros intrínsecos: El punto en el mundo $(X, Y, Z)^{T}$ visto desde la cámara se proyecta en el plano de la imagen como $(fX/Z, fY/Z)^{T}$}}
\end{figure}

Con lo cual los parámetros intrínsecos son: la distancia focal, el punto principal y se agregan el skew (pixel oblicuo), y la distorción de los lentes, etc.

%La cámara y el mundo tienen medidas fisicas como por ejemplo: metros.
%La imagen tiene medidas en pixeles.

La calibración de una cámara es el procedimiento por el cual obtenemos los parámetros intrínsecos y extrínsecos de la cámara a partir de imágenes tomadas por la misma.

%Por otro lado, los parámetros extrínsecos relaciona el sistema de coordenadas del mundo al de la camara. Estos se utilizan %para formar la transformación que mapea los puntos 3D del mundo, a los puntos 2D de la cámara.

Una vez que obtenemos los parámetros intrínsecos y extrínsecos, podemos decir que la
cámara esta calibrada, pudiendo relacionar cualquier punto del mundo real con los puntos proyectados en el plano de la imagen.

%{explicacion de cada figura y decir que si no calibramos cada punto proyectado no se reflejaria bien en la camara, decir que %usamos una camara pinhole}

Para el desarrollo del presente trabajo utilizamos una camara estéreo, con lo cual no nos alcanza solo con calibrar cada una de las cámaras, sino que además debemos calibrar las dos cámaras de forma conjunta. A esto se lo denomina calibración estéreo.

La calibración estéreo nos provee los parámetros intrínsecos de cada cámara (que son los obtenidos en la calibración de cada una) y los parámetros extrínsecos, que nos definen la posición relativa de las cámaras.

Para realizar dicha calibración utilizamos un toolbox de calibración de cámaras para matlab \footnote{http://www.vision.caltech.edu/bouguetj/calib\_doc/}. Esta herramienta utiliza un patrón plano. El mismo debe contener cuadrados blancos y negros intercalados de un cierto tamaño conocido. El proceso del toolbox consiste en capturar imágenes de este patrón desde distintos angulos y luego marcar las cuatro esquinas del patrón en cada una de ellas.

%necesita una serie de fotos para realizar la calibración.

%Como dijimos anteriormente, vamos a utilizar fuertemente mapa de disparidad. Para ello necesitamos calibrar las camaras %utilizadas. La razón por la cual calibramos es para obtener datos feacientes (reales) de las distancias de las camaras, ya sea %en profundidad como también sobre el plano proyectado. La calibración se realiza en dos etapas:

%Primera: se realiza calibracion sobre el plano proyectado. Para ello se utiliza un damero de 30 cm y tomamos distintas %fotografias moviendo la perspectiva del damero. Luego con la herramienta Matlab marcamos las distintas puntas del damero de %cada fotografia. Procesamos estas imagenes con la herramienta y nos devuelve los parametros incrinsicos (CHEQUEAR) de la %camara. Estos nos serviran para poder vincular un punto de una imagen con correspondencia con el otro.

%Segundo: se realiza calibración sobre dos imagenes tomadas en el momento de utilizar el programa de navegación. Esto se %realiza para poder saber la profundidad obtenida por las camaras. Esto se debe realizar en cada momento de correr el software %porque varia considerablemente el mapa de disparidad. Si este paso esta descalibrado; el mapa de disparidad, que es la base %del algoritmo, va a mostrar datos erroneos.  

%mencion de la herramienta: poner link

%Importancia de la calibracion:
%1 porq hay q calibrar las camaras
%2 parametros intrinsicos extrinsicos
\section{Imagen estéreo}
Al proyectarse los objetos, de un espacio tridimensional a una imagen bidimensional se pierde la información de la distancia a la cámara o profundidad (eje Z) de cada punto. Una forma de tratar de recuperar esta información es mediante el uso de dos cámaras, en lo que se conoce como visión estéreo.

A continuación explicaremos la configuración geométrica que se aplica en el campo de visión estéreo. La misma se conoce como geometría epipolar.

\subsection{Geometría epipolar}

%(agregamos figura 12-8 del mismo libro)

%Para entender un poco mas como encuentra la relación entre cada punto que representan el mismo objeto vamos a ver la geometría %se sucede en las cámaras. 
La geometría epipolar posee dos sistemas de referencias, uno por cada cámara, y surgen dos puntos importantes llamados epipolos. Un epipolo es la imagen del centro óptico de una de las cámaras con respecto a la otra (ver figura 5).

\begin{figure}[h]
	\centering
		\includegraphics[width=0.50\textwidth]{./figuras/fig11.png}
	\caption{Geometría epipolar}
	\label{3.1}
\end{figure}

La línea que intersecta los dos centros ópticos $O_{l}$ y $O_{r}$ se llama línea base. Además los epipolos $e_{l}$ y $e_{r}$ surgen de la intersección de la línea base con el plano de la imagen de cada cámara.

Los puntos $O_{l}$, $O_{r}$ y $P$ definen un plano llamado plano epipolar. Las proyecciones $p_{l}$ y $p_{r}$ así como los epilpolos $e_{l}$ y $e_{r}$ pertenecen al plano epipolar. 

Las líneas epipolares son las intersecciones del plano epipolar con los planos de las imagen de cada cámara. Para cualquier plano epipolar, las líneas epipolares incluyen los epipolos.

La imagen de las ubicaciones posible de un punto visto en una de las imágenes es la línea que va desde el correspondiente punto y el punto epipolar en la otra imagen. Es decir, viendo la figura 6 el punto $X$ en el mundo va a estar proyectado en la imagen, de la cámara derecha, en la línea epipolar $l^{'}$. Esta es la utilidad de los epipolos.

\begin{figure}[h]
	\centering
		\includegraphics[width=0.25\textwidth]{./figuras/fig4.png}
	\caption{Geometría epipolar}
	\label{3.1}
\end{figure}

%Dado un único punto P en el mundo puede ser representado por dos vectores diferentes uno en cada sistema de referencia.
%El punto P resulta en dos puntos proyectados, uno en cada imagen.
%Un punto p de la imagen 1 puede ser la proyección de cualquier punto $P_{1}$, $P_{2}$, etc. de la línea $O_{1}P$. Entonces el %punto correspondiente en la imagen 2 debe ser una de las proyecciones $p_{1}$, $p_{2}$, etc. El conjunto de esos puntos %posibles en la imagen 2 se llama línea epipolar. De manera similar se obtiene una línea epipolar en la imagen 1.

%Las líneas epipolares son las intersecciones del plano epipolar con los planos de las imagen de cada cámara.

%Un epipolo es la intersección de todas las líneas epipolares de una imagen.

%La figura 3.1 nos muestra los planos que observa cada cámara y su relación con el punto en común P. Lo mas interesante es ver %que para todo punto que se ve en las dos imágenes existe una linea epipolar que atraviesa por un punto epipolo y que tiene su %correlación en la otra imagen. Esta es la base de la correspondencia entre cada imagen.

%Cada punto en 3D visto desde las cámaras esta contenido en un plano epipolar que intersecta cada imagen en la linea epipolar. %Cuando un punto se corresponde con otro en la otra imagen debe apoyarse sobre su linea epipolar correspondiente. Gracias a %esta regla, la búsqueda de cada punto con respecto a su correspondencia en la otra imagen se reduce de una búsqueda en dos %dimensiones a una busqueda en una dimension , o sea sobre una linea. Esto reduce el computo significativamente.  

%(redactar mejor , la idea es esa)
     

%introduccion: para que nos sirve el mapa de disparidad

%geometria epipolar:
% grafico geometria epipolar con su explicacióm

%como calculamos (como lo hace libelas) el mapa de disparidad (la teoria)

%mencionar libelas (referencia)

%teoria 417 open cv
%implementacion 438 ... 442


%Libelas\footnote{http://www.rainsoft.de/software/libelas.html}
%(basado en Learing Open cv pag 415 Stereo Imaging)

\subsection{Mapas de disparidad}

%(agregar imagenes 12-5 pag 417 libro open cv y 12-6 pag 418 del mismo libro, tambien puede ir la imagen 12-4)
El algoritmo de navegación realizado en el presente trabajo utiliza información de la profundidad que existe entre los puntos del mundo y las cámaras, precisamente al centro óptico de las mismas.
%la distancia que existe entre los puntos del mundo y los centros ópticos de las cámaras.

Una forma de obtener la distancia a la que se sitúa físicamente un objeto en el mundo con respecto a las dos cámaras, consiste en identificar en ambas imágenes aquellos píxeles que se corresponden con la misma entidad física en la escena 3D para luego hallar la distancia que separa estos píxeles. Esto es lo que se conoce como disparidad.

Para calcular la disparidad hemos usado la libreira Libelas\footnote{http://www.rainsoft.de/software/libelas.html}(Library for Efficient Large-scale Stereo Matching). Básicamente, lo que hace esta librería es buscar los descriptores característicos de cada imagen con una implementación del algoritmo SURF\footnote{ftp://ftp.vision.ee.ethz.ch/publications/articles/eth\_biwi\_00517.pdf}(Speeded Up Robust Features) para luego buscar las correspondencias entre los puntos encontrados en el paso anterior. Con lo cual, la disparidad es d = $x_{l}$ - $x_{r}$ donde $x_{l}$ y $x_{r}$ son las correspondencias entre los puntos de cada imagen. 

%una vez que calculamos la disparidad utilizando geometría podemos calcular la distancia Z,

Luego, conociendo el valor de disparidad y la distancia entre los centros ópticos de las cámaras(línea base) podemos calcular la profundidad de los puntos usando triangulos semejantes (ver figura 3.2).

\[
\frac{T - (x_{l} - x_{r})}{Z - f} =\frac{T}{Z} \Rightarrow Z = \frac{fT}{x_{l} - x_{r}} \Rightarrow Z = \frac{fT}{d}
\]

\begin{figure}[h]
	\centering
		\includegraphics[width=0.50\textwidth]{./figuras/fig1.png}
	\caption{figura 1:}
	\label{fig:fig1}
\end{figure}

De la fórmula anterior podemos observar que la distancia de un punto a las cámaras es inversamente proporcional al valor de disparidad (ver figura 3.3). Si el valor de disparidad es cercano a cero, pequeñas diferencias en la disparidad producen grandes diferencias en la profundidad. Y si la disparidad es grande pequeñas diferencias en la disparidad no producen diferencias en la profundidad. La relación de estas dos variables es claramente no lineal.

Con lo cual, si el valor de disparidad es muy pequeño o muy grande la disparidad no nos dice mucho sobre la profundidad a la que se encuentra el objeto de las cámaras. En consecuencia los sistemas de visión estéreo tienen buena resolución de profundidad cuando los objetos se encuentran relativamente cerca.

%Es por eso que no es conveniente que el robot se acerque mucho a un objeto porque la información de disparidad no es útil.

\begin{figure}[h]
	\centering
		\includegraphics[width=0.50\textwidth]{./figuras/fig2.png}
	\caption{Figura 1}
	\label{fig:fig1}
\end{figure}

Cabe aclarar que para conseguir el mejor resultado posible eliminaremos la distorsion radial y tangencial de las imagenes obtenidas con la cámara Minoru. Además se ajusta los ángulos y distancias entre las cámaras para que los planos de las imagenes queden coplanares\footnote{Puntos o líneas que se encuentran en el mismo plano.}. A este último paso se lo conoce con el nombre de rectificacion. Estas operaciones las realizamos con la librería OpenCV.
%La medida de la disparidad sirve para obtener la distancia a la que se sitúa físicamente ese objeto en la escena con respecto %a las dos cámaras

%Realizando los pasos anteriores sucesivamente obtenemos el mapa de disparidad. Dicha imagen nos permite utilizar la %información obtenida de las dos imágenes para saber la profundidad de cada objeto (o punto??). Veamos el concepto que se %utiliza para poder armar esta imagen.

%Asumamos que la imagen no posee distorsión y esta rectificada. En la figura (12-6) podemos ver la representación de la visión %estéreo. Los puntos Ol y Or representan los centros de la cámara izquierda y derecha respectivamente. Los puntos pl y pr son %los proyectados en cada imagen respectivamente y corresponden a un unico punto P. Este punto debe ser alineado por las dos 5
%imágenes. Si definimos como xl y xr como las coordenadas de x en cada punto respectivamente, podemos definir disparidad como d %= xl - xr.

%Cabe aclarar que para obtener buenos resultados se debe sacar las imágenes al mismo tiempo así como también rectificarlas.

\begin{figure}[h]
	\centering
		\includegraphics[width=0.50\textwidth]{./figuras/fig3.png}
	\label{fig:fig1}
\end{figure}

\subsection{Reproyección de puntos en 2D}
%que es la reproyección
%
Conociendo los parámetros intrínsecos y extrínsecos de las dos cámaras, podemos obtener la posición absoluta en tres dimensiones de los puntos proyectados.

Como mencionamos anteriormente, para conseguir los mejores resultados posibles rectificamos las imágenes. Cuando openCV realiza la rectificación, a la vez calcula la matriz de reproyección Q.

\[ Q = \left( \begin{array}{cccc}
1 & 0 & 0 & -c_{x}\\
0 & 1 & 0 & -c_{y}\\
0 & 0 & 0 & f\\
0 & 0 & -1/T_{x} & (c_{x}-c^{'}_{x})/T_{x}\end{array} \right)\]

Donde $T_{x}$ es la longitud de la línea base, $c_{x}$ y $c_{y}$ son las coordenadas del punto principal en la primer cámara, f es la distancia focal y $c^{'}_{x}$ es la coordenada horizontal del punto principal de la segunda cámara.

Luego podemos proyectar el punto en tres dimensiones realizando la  siguiente multiplicación:

\[ Q \left( \begin{array}{cccc}
x \\
y \\
d \\
1 \end{array} \right) = \left( \begin{array}{cccc}
X \\
Y \\
Z \\
W \end{array} \right) \]

%obtenemos las coordenadas en $P^{3}$ y 
Las coordenadas 3D quedarían: \[ \left( \begin{array}{ccc} X/W, & Y/W, & Z/W\end{array} \right) \]

%\subsection{Calculo del mapa de disparidad}

%En esta seccion explicaremos los pasos ha realizar para obtener un mapa de disparidad  a partir de dos imágenes capturadas por %la cámara estéreo.

%Los pasos son:

%1 - Remover distorsiones radiales y tangenciales del lente; este proceso se llama (en ingles) undistortion

%2 - Ajustar los ángulos y distancias entre imágenes; este proceso se llama rectificación. En este paso ya obtenemos imágenes %rectificadas y alineadas

%3 - Encontrar iguales puntos en las dos imágenes, esto se llama correspondencia. La salida de este proceso es una imagen %llamada mapa de disparidad, donde la disparidad se refiere a la diferencia entre las dos imágenes en un mismo punto.

%4 - Por ultimo, podemos pasar el mapa de disparidad a distancias utilizando triangulación; este proceso se llama reproyección %y la salida es una imagen con profundidad.

%Realizamos estos pasos utilizando fuertemente la librería Open CV. Esta librería nos facilita mucho la realización de cada uno %de ellos. Vamos a dedicarnos fuertemente en explicar como utilizamos el mapa de disparidad.

\section{Navegación para robot móviles utilizando los mapas de disparidad}
%El flujo del algoritmo de navegación es el siguiente: 
A continuación explicaremos brevemente los pasos realizados desde que se capturan las imágenes hasta que el robot realiza el movimiento calculado.

Como primer paso, en un mismo instante, capturamos con cada cámara la escena del mundo. Luego se calcula el mapa de disparidad, habiendo eliminado la distorsión radial y tangencial de las imágenes; así como habiendo realizado la rectificación de las mismas. Una vez que obtuvimos el mapa de disparidad se realiza una heuristica para encontrar el valor medio de disparidad. Con este valor construimos el punto a reproyectar, obteniendo su posición absoluta en el mundo real. Por último calculamos el ángulo de giro y la distancia a recorrer para ir a esta posición y le indicamos al robot que se movilice a dicho lugar.

%En consecuencia un punto que este relativamente cerca del robot, debido que el mapa de disparidad nos brinda información útil %bajo estas condiciones.

A continuación explicaremos en detalle los pasos más importantes del algoritmo.

%%%%%%%
\subsection{Procesamiento del mapa de disparidad}

Los valores de disparidad del algoritmo se asignan con el mapa de disparidad que referencia a las coordenadas de la cámara izquierda.

La región de interes es el área dentro de la cual se buscan los obstaculos. Por lo tanto, el algoritmo esta diseñando para ver los obstaculos solo en esta área particular. El tamaño de la región de interes es de 640 x 40 pixels. Esta región esta determinada entre las filas 300 y 340 de pixels, siendo el tamaño de las imágenes de 640 x 480 pixels.


%Sobre el mapa de disparidad tomaremos una región del mismo la cual llamaremos región de interes.

%El mapa de disparidad nos aporta información sobre la profundidad a la que se encuentran los objetos del robot. Sin embargo, %para agilizar el procesamiento, pero no va a ser necesario utilizarla en su totalidad para decidir que accion tomar.

%Sobre ella nos quedamos con una región. 

%La razon por la cual obtenemos esta porcion de la imagen es que justo incluye la interseccion del suelo con el horizonte. Este %corte horizontal lo utilizamos para poder decidir para que lugar moverse. 

%Realizamos varios experimentos y llegamos a la %conclusión de que es la mejor region para definir el posible camino dada la %arquitectura y las condiciones utilizadas. 

%, con esta vamos a continuar con los pasos de navegacion. %(meter grafico)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Cuando obtuvimos esta porción de imagen, vamos a procesarla para poder tomar una decisión con respecto a la navegación.

%Esta región la llamaremos "región de interes", sobre esta porción del mapa de disparidad aplicaremos la heuristica comentada %anteriormente para encontrar punto al cual dirigirse.

%Para evitar los obstaculos vamos a buscar, en la región de interes, el punto al cual el robot se moverá.

El procedimiento de navegación consiste en buscar un punto en la región de interes, al cual dirigirse, que este relativamente cerca del robot y en el que no se encuentre ningún obstaculo. El mismo se basa en el concepto de la figura 6. El cual nos indica que los valores de disparidad cercanos a cero y los que son muy grandes no nos aportan información útil sobre la distancia que hay del punto a las cámaras. Para lograr esto buscaremos el valor de disparidad medio.

\subsubsection{Heurística del valor de disparidad medio}
%La heuristica presentada para encontrar el valor de diparidad medio, 
La heurística consiste en recorrer la región de interes calculando la mediana de los valores de disparidad de cada columna. Es decir, aplanamos el ROI a una sola fila que contiene en cada celda el valor de disparidad medio de la columna (ver figura). Luego, calculamos la mediana de esta fila consiguiendo el valor de disparidad medio del ROI.

\begin{figure}[h]
	\centering
		\includegraphics[width=0.50\textwidth]{./figuras/fig13.png}
	\caption{Heuristica}
\end{figure}

Esta heurística no nos asegura que el punto encontrado sea el mejor al cual dirigirse; pero sabemos que con dicho valor de disparidad nos brinda información fehaciente de la distancia en la que se encuentra el punto de las cámaras.

Puede ocurrir que el mapa de disparidad no nos provea suficiente información para decidir a que punto ir, ya sea porque el objeto esta demasiado cerca o porque la imagen tiene poca luz. Esto se ve reflejado en que el ROI aplanado posee más de la mitad de los valores fuera de rango. Sí esto sucede se aplica una política de escape ya que la información provista no es suficiente para determinar una desición. La política de escape consiste en girar una cierta cantidad de grados hacia la derecha buscando una escena más apropiada para encontrar un punto al cual dirigirse.

Otra ocasión en la que se aplica la política de escape es cuando el punto calculado por la heurística se encuentra muy cerca del robot. Si bien no es sumamente necesario es una mejora para encontrar mejores caminos. 

%Ya que si nos acercamos mucho a un objeto se aplicará la politica de escape por falta de información en el mapa de disparidad. Y si el objeto que esta cerca es muy grande se tendra que aplicar varias veces la politica de escape probablemente volviendo a pasra por el camino ya recorrido.

%De esta manera, nos quedamos con un valor de disparidad medio que no nos asegura que sea el mejor punto al cual dirigirse pero %sabemos que con dicho valor de disparidad, al proyectarlo, nos indicará la distancia correcta en la que se encuentra el punto %de las cámaras.

%Primero vamos a recorrer la region realizando la mediana de los datos por cada columna de información. (tenemos que hacer un %grafico de esto.) 
%Si la columna posee algún valor muy lejano se le asigna un valor muy grande parametrizado en el programa. Realizamos distintos %experimentos y llegamos a la conclusion que la mediana representa bastante bien la distancia de cada columna. En un principio %utilizamos el promedio de los valores pero fue descartado por lo siguiente:  Supongamos que poseemos 1/3 de la columna con %pixeles negros. El promedio de esta columna va a estar cerca del valor de negro, descartando esa columna de informacion. Si %utilizamos la mediana, los valores mayores van a estar concentrados en la parte final. Al utilizar los dos elementos del medio 5vamos a obtener un valor de disparidad real y no va a estar afectado a los pixeles negros. Es por eso que nos inclinamos a %utilizar la mediana. Muchas veces experimentamos esto por tener imagenes con poca luminosidad por ejemplo y pudimos evitarlo %de esta forma.

%Al finalizar este primer procesamiento tenemos un vector con la mediana de cada una de las colunas.

%Si tenemos informacion suficiente, vamos a ordenar el vector resultante del paso anterior y calculamos su mediana. Si este %valor es mayor que 30 quiere decir que tenemos un objeto muy cerca. La cota que elegimos la obtuvimos de hacer varios %experimentos y llegamos a la conclusion que es lo ideal para la arquitectura que utilizamos.  

\subsection{Calculo del ángulo de giro y distancia a recorrer}

Como explicamos anteriormente, para reproyectar un punto debemos multiplicar la matriz de reproyección Q por el punto a reproyectar que esta definido por: \[ \left( \begin{array}{cccc} x, & y, & d, & 1\end{array} \right) \]

Con lo cual para reproyectar el punto al que se dirigirá el robot necesitamos definir $x$ e $y$. $x$ será la coordenada de la primer celda, del ROI aplanado, que contenga el valor de disparidad calculado por la heuristica (comenzando dede la izquierda), mientras que $y$ será la coordenada vertical inferior del ROI.

Luego reproyectamos este punto para obtener el punto P, en el mundo real, al cual dirigirse.

%Luego de encontrar un punto: \[ \left( \begin{array}{cccc} x, & y, & d, & 1\end{array} \right) \] lo reproyectamos para %obtener el punto P en el mundo real (3D), al cual dirigirse. 

Por último utilizando propiedades trigonométricas calculamos el ángulo de giro y la distancia. Cabe aclara que al robot no se le indica la distancia calculada, sino que se le indica una distancia menor para que no quede muy cerca del objeto y tenga que aplicar la política de escape.
%el ángulo de giro se esta calculando con la siguiente formula:
%angulo = asin(X/Z);
%if(X > 0){
%    angulo = asin(X/Z);
%}
%else{
%    angulo = asin((-1 * X)/Z);
%    angulo = -angulo;
%}
%la distancia se esta calculando con la norma dos del punto (distance = sqrt(X*X + Y*Y + Z*Z))

%analizar si lo expresamos así o lo dejamos simple en la otra subsección.

%Luego de hacer todos estos chequeos, vamos a definir que direccion tomar. Se va a calcular el punto 3D que representa esa %mediana previamente calculada. Al obtener eso podemos saber a cuanta distancia estamos de ese punto. Por ultimo se calcula el %angulo de giro y ya con estos dos datos podemos determinar que camino elegir.

%Una vez que obtuvimos el mapa de disparidad debemos correr nuestro algoritmo de navegación. Dado un mapa de disparidad, vamos %a explicar paso a paso que haría con ella.

%Sobre toda la imagen de disparidad nos quedamos con una región. Esta región esta determinada entre las filas 300 y 340 de la %imagen. Este corte horizontal lo utilizamos para poder decidir para que lugar moverse. Realizamos varios experimentos y %llegamos a la conclusión de que es la mejor region para definir el posible camino dada la arquitectura y las condiciones %utilizadas. Esta región la llamaremos región de interes, con esta vamos a continuar con los pasos de navegacion.

%Cuando obtuvimos esta porción de imagen, vamos a procesarla para poder tomar una decisión con respecto a la navegación. %Recorremos toda la imagen por columnas realizando la mediana de los datos por cada columna de información. (tenemos que hacer %un grafico de esto.) Si la columna posee algún valor muy lejano se le asigna un valor muy grande parametrizado en el programa. %Realizamos distintos experimentos y llegamos a la conclusion que la mediana  representa bastante bien la distancia de cada %columna. En un principio utilizamos el promedio de los valores pero fue descartado por lo siguiente: Supongamos que poseemos %1/3 de la columna con pixeles negros. El promedio de esta columna va a estar cerca del valor de negro, descartando esa columna %de informacion. Si utilizamos la mediana, los valores mayores van a estar concentrados en la parte final. Al utilizar los dos %elementos del medio vamos a obtener un valor de disparidad real y no va a estar afectado a los pixeles negros. Es por eso que %nos inclinamos a utilizar la mediana. Muchas veces experimentamos esto por tener imagenes con poca luminosidad por ejemplo y %pudimos evitarlo de esta forma.

%Luego de realizado este procesamiento sobre cada columna de la imagen, obtenemos un array de todas las distancias sugeridas %por cada columna.

%Si no se obtiene una cadena de datos con información suficiente como para elegir un camino a seguir, el algoritmo define una %política de escape.

%region de interes
%funcion de mediana
%justificacion
%puntos 3D
%arquitecteura de software y de hardware(figura 3 y 4 paper base)

\section{Experimentos}
%Se realizaron pruebas para averiguar la velocidad y velocidad de giro del robot. por ejemplo 2 cm por segundo, 10 grados por segundo.

El entorno en que probamos el algoritmo de navegación es en ambientes interiores.

%El entorno donde hicimos la mayoría de las pruebas es bastante particular. 
El piso no tiene ninguna diferencia de nivel, la luminosidad era bastante importante y los objetos a esquivar fueron cajas grandes con aristas bien definidas. 

%Estas condiciones son clave para poder obtener los resultados que antes expusimos.

Si el ambiente es luminoso se consiguen mejores resultados porque la calidad de las imágenes será mejor.

%Los valores de cotas escritas en el codigo del programa fueron obtenidas en la sucesion de experimentos realizados en este %entorno.
\subsection{Arquitectura}
%lugares donde se probo. Pruebas realizadas
La arquitectura utilizada se puede dividir en dos partes: la computadora donde se corre el algoritmo descripto y la mecánica del robot. La computadora utilizada fue una Netbook Asus con sistema operativo Ubuntu Linux. La capacidad de procesamiento fue mas que suficiente para la ejecucion del mismo, entonces podemos afirmar que nuestro algoritmo no posee altas restricciones de performance. La mecanica del robot se basa en un controlador que puede controlar las ruedas. Estas son dos de cada lado unidas por horugas. Para poder comunicarse con esta placa enviamos paquetes udp desde la Netbook. Para la parte de vision utilizamos la camara stereo Minoru que se encuentra conectada por usb al frente de la Netbook.  

La conexion de nuestro programa con estas dos interfaces se realiza gracias a librerias. Utilizamos la libreria Libcam que nos permite sacar buenas fotos con esta camara stereo. Para la parte de comunicacion con las ruedas utilizamos la libreria libexabot-remote.

\subsection{Limitaciones}
%limitaciones encontradas (obstaculos grandes, luminosidad, objetos fijos, indoor, distancia de acercamiento)
La limitacion se basa fuertemente en el entorno que utilizamos. Si no cumplimos con el entorno que se utilizo en experimentos no se va a poder asegurar los resultados obtenidos.

Otra limitacion importante es que los obstaculos no deben moverse, esto hace mas facil que nuestro algoritmo haga su trabajo. 

Cabe destacar que el entorno utilizado juega un papel fundamental en el buen desempeño del algoritmo, utilizamos un entorno llano, iluminado para no perder ningún dato al tomar cada fotografía. La cámara se encuentra en la parte superior del frente del robot y su movilidad se reduce a avanzar, retroceder y rotar sobre su propio eje.

\section{Trabajos Futuros}

%mapas vistos de arriba
%mejorar algunas cosas
%utilizar la información obtenidas de las imágenes anteriores para tomar mejores decisiones.(con objetos fijos)
%thread(futuro cercano y lejano a la vez)

Nuestro trabajo nos provee un buen puntapie para seguir con un desarrollo mas sofisticado. Se podria mejorar en varios aspectos, por ejemplo poder ir armando un mapa visto desde el techo con cada movimiento realizado con el robot. Esto seria muy util porque nos mostraria el caminorealizado por el robot.

Otra mejora podria ser tener un buffer de las imagenes anteriores y poder utilizarlas de alguna forma para tomar la nueva decision de la direccion a seguir.

Otra mejora mas especifica podria ser utilizar un thread aparte para sacar las fotos. Este thread sacaria imagenes constantemente y las dejaria en una pila y cuando el algoritmo quera decidir un nuevo camino para continuar podria desapilar las imagenes que necesite y tomar la desicion.

\section{Conclusión}
El presente trabajo es una idea simple de como poder llevar a cabo un algoritmo de navegacion en una arquitectura definida. Resulto muy interesante ver como entre sucesivos experimentos pudimos mejorar el algoritmo. Tambien nos sirve para abordar muchos mas temas interesantes sobre vision robotica.

\end{document}
